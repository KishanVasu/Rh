{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang16393{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.10586}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\par
% LaTeX Example: Project Report\par
%\par
% Source: {{\field{\*\fldinst{HYPERLINK http://www.howtotex.com }}{\fldrslt{http://www.howtotex.com\ul0\cf0}}}}\f0\fs22\par
%\par
% Feel free to distribute this example, but please keep the referral\par
% to howtotex.com\par
% Date: March 2011 \par
% \par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\par
% How to use writeLaTeX: \par
%\par
% You edit the source code here on the left, and the preview on the\par
% right shows you the result within a few seconds.\par
%\par
% Bookmark this page and share the URL with your co-authors. They can\par
% edit at the same time!\par
%\par
% You can upload figures, bibliographies, custom classes and\par
% styles using the files menu.\par
%\par
% If you're new to LaTeX, the wikibook is a great place to start:\par
% {{\field{\*\fldinst{HYPERLINK http://en.wikibooks.org/wiki/LaTeX }}{\fldrslt{http://en.wikibooks.org/wiki/LaTeX\ul0\cf0}}}}\f0\fs22\par
%\par
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\par
% Edit the title below to update the display in My Documents\par
%\\title\{Project Report\}\par
%\par
%%% Preamble\par
\\documentclass[paper=a4, fontsize=12pt]\{scrartcl\}\par
\\usepackage[T1]\{fontenc\}\par
\\usepackage\{fourier\}\par
\par
\\usepackage[english]\{babel\}\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab % English language/hyphenation\par
\\usepackage[protrusion=true,expansion=true]\{microtype\}\tab\par
\\usepackage\{amsmath,amsfonts,amsthm\} % Math packages\par
\\usepackage[pdftex]\{graphicx\}\tab\par
\\usepackage\{url\}\par
\\linespread\{1.3\}\par
%\\graphicspath\{ \{c:/Users/\} \}\par
%%% Custom sectioning\par
\\usepackage\{sectsty\}\par
\\allsectionsfont\{\\centering \\normalfont\\scshape\}\par
\\usepackage[colorinlistoftodos]\{todonotes\}\par
\par
%%% Custom headers/footers (fancyhdr package)\par
\\usepackage\{fancyhdr\}\par
\\pagestyle\{fancyplain\}\par
\\fancyhead\{\}\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab % No page header\par
\\fancyfoot[L]\{\}\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab % Empty \par
\\fancyfoot[C]\{\}\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab\tab % Empty\par
\\fancyfoot[R]\{\\thepage\}\tab\tab\tab\tab\tab\tab\tab\tab\tab % Pagenumbering\par
\\renewcommand\{\\headrulewidth\}\{0pt\}\tab\tab\tab % Remove header underlines\par
\\renewcommand\{\\footrulewidth\}\{0pt\}\tab\tab\tab\tab % Remove footer underlines\par
\\setlength\{\\headheight\}\{13.6pt\}\par
\par
\par
%%% Equation and float numbering\par
\\numberwithin\{equation\}\{section\}\tab\tab % Equationnumbering: section.eq#\par
\\numberwithin\{figure\}\{section\}\tab\tab\tab % Figurenumbering: section.fig#\par
\\numberwithin\{table\}\{section\}\tab\tab\tab\tab % Tablenumbering: section.tab#\par
\par
\par
%%% Maketitle metadata\par
\\newcommand\{\\horrule\}[1]\{\\rule\{\\linewidth\}\{#1\}\} \tab % Horizontal rule\par
\par
\\title\{\par
\tab\tab %\\vspace\{-1in\} \tab\par
\tab\tab\\usefont\{OT1\}\{bch\}\{b\}\{n\}\par
\tab\tab %\\normalfont \\normalsize \\textsc\{School of random department names\} \\\\ [25pt]\par
\tab\tab\\horrule\{0.5pt\} {{\field{\*\fldinst{HYPERLINK "\\\\\\\\[0.4cm"}}{\fldrslt{\\\\[0.4cm\ul0\cf0}}}}\f0\fs22 ]\par
\tab\tab\\huge Chapter 1 \\\\ Introduction \\\\\par
\tab\tab\\horrule\{2pt\} {{\field{\*\fldinst{HYPERLINK "\\\\\\\\[0.5cm"}}{\fldrslt{\\\\[0.5cm\ul0\cf0}}}}\f0\fs22 ]\par
\}\par
\par
\\date\{\}\par
%%% Begin document\par
\\begin\{document\}\par
\\maketitle\par
\\newpage\par
\par
\\section\{Introduction\}\par
 There are two primary methods in traditional computing for the execution of algorithms.  The first is to use an Application Specific Integrated Circuit (ASIC), to perform the operations in hardware.  Because these ASICs are designed specifically to perform a given computation, they are very fast and efficient when executing the exact computation for which they were designed.  However, the circuit cannot be altered for any other computation.  Microprocessors are a far more flexible solution.  Processors execute a set of instructions to perform a computation.  By changing the software instructions, the functionality of the system is altered without changing the hardware.  However, the downside of this flexibility is that the performance suffers, and is far below that of an ASIC.  The processor must read each instruction from memory, determine its meaning, and only then execute it.  This results in a high execution overhead for each individual operation[1]. \par
\par
 Reconfigurable computing (RC) is intended to fill the gap between hardware and software, achieving potentially much higher performance than software, while maintaining a higher level of flexibility than hardware. Typical RC systems yield 10X to 100X improvement in processing speed over conventional CPU-based "software- only" systems [2]. RC systems merge the advantages of ASICs and General purpose processors.Fig.1 represents the attributes of RC computing with respect to Processor and ASIC.  \par
 \par
 \\begin\{center\}\par
\tab\tab\\centering\par
\tab\tab\\includegraphics[scale=1.5]\{1.png\}\par
\tab\tab\\newline\par
\tab\tab\\centering\par
\tab\tab Figure 1 : Flexibility vs  Data-Processing rate \par
\\end\{center\}\par
\par
 Partial Reconfiguration (PR) or Run-Time Reconfiguration (RTR) is implemented. It is defined as the ability to modify or change the functional configuration of the device during operation, through either hardware or software changes. PR is based upon the concept of virtual hardware, which is similar to virtual memory. The physical hardware is much smaller than the sum of the resources required by each of the configurations. Therefore, instead of reducing the number of configurations that are mapped, we instead swap them in and out of the actual hardware as they are needed[3]. \par
 \par
 Fig.2 gives an overall picture of how the reconfigurable modules are swapped in and out of the reconfigurable partition which will be created during the floorplanning.\par
\par
\\begin\{center\}\par
\tab\tab\\centering\par
\tab\tab\\includegraphics[scale=1.5]\{1.png\}\par
\tab\tab\\newline\par
\tab\tab\\centering\par
\tab\tab Figure 2 : Partial dynamically reconfigurable architecture \par
\\end\{center\}\par
\par
  To demonstrate the working of partially reconfigurable design a naval application is developed. \par
  \par
\\subsection\{Automatic Target Recognition\}\par
 Automatic Target Recognition (ATR) is ability of an algorithm or device to recognize the targets based on the data received by the sensors. Target recognition can be done in various ways, namely audio representation of the signal, visual representation of the data etc. Here, the visual representations of the signal i.e. images are used for target recognition. In real-time scenarios, the images from Forward Looking Infrared (FLIR) sensor are used for target recognition. The datasets are generated using MATLAB, Photoshop and Picasa.\par
 \par
 In recognition problems, there are two major steps, namely feature extraction and feature classification.\par
Fig.3 shows the basic block diagram of a target recognition system.\par
\par
\\begin\{center\}\par
\tab\tab\\centering\par
\tab\tab\\includegraphics[scale=1.5]\{1.png\}\par
\tab\tab\\newline\par
\tab\tab\\centering\par
\tab\tab Figure 3 : A basic block diagram for target recognition \par
\\end\{center\}\par
\par
 Feature extraction is employed when the input data in an application is too large to be processed and is redundant. In this case the input data will be raw pixels. In order to obtain the reduced representation of the initial data, feature extraction is used. Hence, feature extraction is closely related to dimensionality reduction. \par
 \par
 Principal Component Analysis is one of the most efficient techniques to achieve dimensionality reduction and get non-redundant representation of a large data. It uses an Information Theory approach wherein the most relevant image information is encoded in a group of images that will best distinguish every image. It transforms the target and clutter images in to a set of basis images, which essentially are the principal components of the images. The Principal Components (or Eigenvectors) basically seek directions in which it is more efficient to represent the data. This is particularly useful for reducing the computational effort.\par
 \par
 Such an information theory approach will encode not only the local features but also the global features. When we find the principal components or the Eigenvectors of the image set, each Eigenvector has some contribution from each image used in the training set.\par
\par
 Every image in the training set can be represented as a weighted linear combination of these basis matrices. The number of Eigen images that we obtain therefore would be equal to the number of images in the training set. Let that number be M. Some of these Eigen images are more important in encoding the variation in images of the dataset, thus we could also approximate all images using only the K most significant Eigen images.\par
 \par
 Classification is done by the 2-class neural network classifier. During the training, the neural network learns the weights and the bias terms from the training data, which will be the feature vectors of every image. There are three layers, namely input layer, hidden layer and output layer.Fig.4 shows the basic architecture of the feedforward Artificial Neural Network (ANN).\par
\par
\\begin\{center\}\par
\tab\tab\\centering\par
\tab\tab\\includegraphics[scale=1.5]\{1.png\}\par
\tab\tab\\newline\par
\tab\tab\\centering\par
\tab\tab Figure 4 : Basic Architecture of Artificial Neural Network\par
\\end\{center\}\par
\par
The circle in the above figure is the neuron which is the basic building block of a neural network.\par
 \par
 The number of neurons can be varied based on recognition problem. The number of neurons in the input layer depends upon the dimension of the feature vector. The number of neurons in the hidden layer can be varied such that desired level of accuracy is achieved. The number of classes decides the number of neurons in the output layer. \par
\par
\\subsection\{Motivation\}\par
 Nowadays, the demands for FPGA-based embedded systems with higher performance in terms of powerful computational ability and fast processing time are rising rapidly.\par
Latest applications ported to embedded systems (e.g., pattern recognition, scalable video rendering, communication protocols) demand a large computation power, while must respect other critical embedded design constraints, such as, short time-to-market, low energy consumption or reduced implementation size. Increasing number of processors does not always translate to linear speedup because not all portions of the application can be parallelized. Here is where the dynamically loading and unloading modules (PR) at run time can be an alternative over the existing methods.\par
\par
 The speed of RC systems is much greater than conventional software systems. Another compelling advantage is reduced energy and power consumption. In a recon\f1\u-1279?gurable system, the circuitry is optimized for the application, such that the power consumption will tend to be much lower than that for a general-purpose processor. Various surveys report that moving critical software loops to recon\u-1279?gurable hardware results in average energy savings of 35\\% to 70\\% with an average speed up of 3 to 7 times, depending on the particular device used. Other advantages of recon\u-1279?gurable computing include a reduction in size and component count (and hence cost), improved time-to-market, and improved \u-1278?exibility and upgradability. These advantages are especially important for embedded applications [4]. \par
\\newpage\par
\par
\\title\{\par
\tab\tab %\\vspace\{-1in\} \tab\par
\tab\tab\\usefont\{OT1\}\{bch\}\{b\}\{n\}\par
\tab\tab %\\normalfont \\normalsize \\textsc\{School of random department names\} \\\\ [25pt]\par
\tab\tab\\horrule\{0.5pt\} {{\field{\*\fldinst{HYPERLINK "\\\\\\\\[0.4cm"}}{\fldrslt{\\\\[0.4cm\ul0\cf0}}}}\f1\fs22 ]\par
\tab\tab\\huge Chapter 2 \\\\ Project Deliverables \\\\\par
\tab\tab\\horrule\{2pt\} {{\field{\*\fldinst{HYPERLINK "\\\\\\\\[0.5cm"}}{\fldrslt{\\\\[0.5cm\ul0\cf0}}}}\f1\fs22 ]\par
\}\par
\\maketitle\par
\\newpage\par
\par
\\section\{Project Deliverables\}\par
\\subsection\{Embedded Design of reconfigurable hardware using EDK and Xilinx PlanAhead\}\par
\\begin\{itemize\}\par
\tab\\item Design with single memory module and controllers. \par
    \\item Design with two memory modules connected in parallel to achieve enhanced memory for the software application.\par
\\end\{itemize\}\par
\par
\\subsection\{Implementation of PCA and ANN in MATLAB.\}\par
\\begin\{itemize\}\par
\tab\\item The training and testing code written in MATLAB.\par
    \\item Trained weights and biases are saved in IEEE 754 format to text files.\par
\\end\{itemize\}\par
\par
\\subsection\{Implementation of PCA and ANN in Xilinx C\}\par
\\begin\{itemize\}\par
\tab\\item Testing Code is written in Xilinx C using the reconfigurable hardware.\par
    \\item The accuracy of the results is verified with MATLAB results.\par
\\end\{itemize\}\par
\\newpage\par
\par
\par
\\title\{\par
\tab\tab %\\vspace\{-1in\} \tab\par
\tab\tab\\usefont\{OT1\}\{bch\}\{b\}\{n\}\par
\tab\tab %\\normalfont \\normalsize \\textsc\{School of random department names\} \\\\ [25pt]\par
\tab\tab\\horrule\{0.5pt\} {{\field{\*\fldinst{HYPERLINK "\\\\\\\\[0.4cm"}}{\fldrslt{\\\\[0.4cm\ul0\cf0}}}}\f1\fs22 ]\par
\tab\tab\\huge Chapter 3 \\\\ Background \\\\\par
\tab\tab\\horrule\{2pt\} {{\field{\*\fldinst{HYPERLINK "\\\\\\\\[0.5cm"}}{\fldrslt{\\\\[0.5cm\ul0\cf0}}}}\f1\fs22 ]\par
\}\par
\\maketitle\par
\\newpage\par
\par
\\section\{Background\}\par
 This chapter begins with a more detailed description of Partial Reconfiguration. The underlying equations in the image processing algorithms are explained in this chapter.\par
 \par
\\subsection\{Partial Reconfiguration\}\par
 Partial Reconfiguration (PR) is modifying a subset of logic in an operating FPGA design by downloading a partial configuration file via Internal Configuration Access Port (ICAP)[1].\par
 \par
 FPGA technology provides the flexibility of on-site programming and re-programming without going through re-fabrication with a modified design. PR takes this flexibility one step further, allowing the modification of an FPGA design during run-time by loading a partial configuration file, usually a partial BIT file. After a full BIT file configures the FPGA, partial BIT files can be downloaded to modify reconfigurable regions in the FPGA without compromising the integrity of the applications running on those parts of the device that are not being reconfigured.\par
 \par
 Figure 5 illustrates the premise behind Partial Reconfiguration.\par
\par
\\begin\{center\}\par
\tab\tab\\centering\par
\tab\tab\\includegraphics[scale=1.5]\{1.png\}\par
\tab\tab\\newline\par
\tab\tab\\centering\par
\tab\tab Figure 5 : Basic Premise of  Partial Reconfiguration\par
\\end\{center\}\par
\par
 As shown, the function implemented in Reconfig Block A is modified by downloading one of several partial BIT files, A1.bit, A2.bit, A3.bit, or A4.bit. The logic in the FPGA design is divided into two different types, reconfigurable logic and static logic. The gray area of the FPGA block represents static logic and the block portion which is labeled Reconfig Block \ldblquote A\rdblquote  represents reconfigurable logic.\par
 \par
 The static logic remains functioning and is completely unaffected by the loading of a partial BIT file. The reconfigurable logic is replaced by the contents of the partial BIT file.\par
 \par
 The basic premise of Partial Reconfiguration is that the FPGA hardware resources can be time-multiplexed similar to the ability of a microprocessor to switch tasks. Because the FPGA device is switching tasks in hardware, it has the benefit of both flexibility of a software implementation and the performance of a hardware implementation.\par
\par
 There are many reasons why the ability to time multiplex hardware dynamically on a single FPGA device is advantageous. \par
 These include: \par
 \par
\\begin\{itemize\}\par
\tab\\item Reducing the size of the FPGA device required to implement a given function, with consequent reductions in cost and power consumption. \par
    \\item Providing flexibility in the choices of algorithms or protocols available to an application \bullet  Enabling new techniques in design security.\par
    \\item Improving FPGA fault tolerance.\par
    \\item Accelerating configurable computing.\par
\\end\{itemize\}\par
\par
\par
\\subsection\{Principle Component Analysis\}\par
 In order to implement target recognition, PCA[1]. is used for feature extraction. The algorithm can be enumerated as follows,\par
 \par
\\begin\{center\}\par
\tab\tab\\centering\par
\tab\tab\\includegraphics[scale=1.5]\{1.png\}\par
\tab\tab\\newline\par
\tab\tab\\centering\par
\tab\tab Figure 1 : Image\par
\\end\{center\}\par
\\newpage\par
\par
\\newpage\par
\par
\\subsection\{Artificial Neural Network\}\par
 After obtaining the features of the images, they are classified using an ANN[2.]. Neural networks are typically organized in layers. Layers are made up of a number of interconnected 'nodes' which contain an 'activation function'. Patterns are presented to the network via the 'input layer', which communicates to one or more 'hidden layers' where the actual processing is done via a system of weighted 'connections'. The hidden layers then link to an 'output layer' where the answer is output.\par
 \par
 Most ANNs contain some form of \lquote learning rule\rquote  which modifies the weights of the connections according to the input patterns that it is presented with. We make use of \lquote delta rule\rquote  for backwards propagation of errors. With the delta rule, as with other types of backpropagation, 'learning' is a supervised process that occurs with each cycle or 'epoch' (i.e. each time the network is presented with a new input pattern) through a forward activation flow of outputs, and the backwards error propagation of weight adjustments. In other words, when a neural network is initially presented with a pattern it makes a random guess as to what it might be.\par
 \par
 It then sees how far its answer was from the actual one and makes an appropriate adjustment to its connection weights. Also, within each hidden layer node is a hyperbolic tangent (tanh) activation function which polarizes network activity, adds non-linearity to the inputs and helps it to stabilize. Backpropagation performs a gradient descent within the solution's vector space towards a 'global minimum' along the steepest vector of the error surface. The global minimum is that theoretical solution with the lowest possible error.\par
Learning Rate is multiplied by the error and then weights are updated. Learning rate helps the network to overcome obstacles (local minima) in the error surface and settle down at or near the global minimum.\par
\par
 Once a neural network is 'trained' to a satisfactory level it may be used as an analytical tool on other data. To do this, the user no longer specifies any training runs and instead allows the network to work in forward propagation mode only. New inputs are presented to the input pattern where they filter into and are processed by the middle layers as though training were taking place, however, at this point the output is retained and no back propagation occurs. The output of a forward propagation run is the predicted model for the data which can then be used for further analysis and interpretation.\par
\par
The brief steps for implementation of a feed forward ANN are as follows:\par
\\begin\{itemize\}\par
\tab\\item The weights are randomly initialized which will prevent the network to be stuck in local minima while updating weights.\par
\tab\\item During forward propagation through a network, the output (activation) of a given node is a function of its inputs. The inputs to a node, which are simply the products of the output of preceding nodes with their associated weights, are summed and then passed through an activation function before being sent out from the node. \par
    <<formula>>\par
 where  Sj  is the sum of all relevant products of weights and outputs from the previous layer i, wij represents the relevant weights connecting layer i with layer j, ai represents the activations of the nodes in the previous layer i, aj is the activation of the node at hand, and f is the activation function.\par
 \par
 In this application case tanh function is used as an activation function. \par
\tab\\item The error function is commonly given as the sum of the squares of the differences between all target and actual node activations for the output layer. For a particular training pattern (i.e., training case), error is thus given by  \par
\tab\par
    <<formula>>\par
    \par
 where Ep is total error over the training pattern, \f0\'bd is a value applied to simplify the function\rquote s derivative, n represents all output nodes for a given training pattern, tjn represents the target value for node n in output layer j, and ajn represents the actual activation for the same node.\par
 \par
 Error over an entire set of training patterns (i.e., over one iteration, or epoch) is calculated by summing all Ep is given by \par
\par
\tab <<formula>>\par
   \par
where E is total error, and p represents all training patterns. \par
\tab\\item Gradient descent learning uses this error function for the modification of weights along the most direct path in weight-space to minimize error, change applied to a given weight is proportional to the negative of the derivative of the error with respect to that weight. The negative of the derivative of the error function is required in order to perform gradient descent learning.\par
    \\item Backpropagation Algorithm is used to update the weights between input layer and hidden layer as well as weights between hidden layer and output layer.\par
\tab\\item By using the chain rule,\par
\par
\tab <<formula>>\par
   \par
 where, \'80 is the learning rate. A higher value for \'80 will necessarily result in a greater magnitude of change. Because each weight update can reduce error only slightly, many iterations are required in order to satisfactorily minimize error. \par
 \par
 \tab <<formula>>\par
    \par
\'80 is the learning rate.\par
Backpropagation and updating of weights is done over large epochs and the error almost approaches zero. When the error in the network is negligibly small, then the network is said to have learnt the pattern from the training set.\par
Once the neural network is trained, the weight matrix will be saved. Given a test image, recognition will be done using the weights and biases learnt during training.\par
The following is done during testing,\par
\tab\\item During testing, only forward propagation is implemented. The following equations are implemented.\par
 \tab\\item Based on the magnitude of the final result, the classification is done.\par
\par
\par
    \par
\par
\\end\{itemize\}\par
    \par
\\begin\{align\} \par
\tab\\begin\{split\}\par
\tab (x+y)^3 \tab &= (x+y)^2(x+y)\\\\\par
\tab\tab\tab\tab\tab &=(x^2+2xy+y^2)(x+y)\\\\\par
\tab\tab\tab\tab\tab &=(x^3+2x^2y+xy^2) + (x^2y+2xy^2+y^3)\\\\\par
\tab\tab\tab\tab\tab &=x^3+3x^2y+3xy^2+y^3\par
\tab\\end\{split\}\tab\tab\tab\tab\tab\par
\\end\{align\}\par
Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies \par
\par
\\subsection\{Heading on level 2 (subsection)\}\par
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. \par
\\begin\{align\}\par
\tab A = \par
\tab\\begin\{bmatrix\}\par
\tab A_\{11\} & A_\{21\} \\\\\par
  \tab A_\{21\} & A_\{22\}\par
\tab\\end\{bmatrix\}\par
\\end\{align\}\par
Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem.\par
\par
\\subsubsection\{Heading on level 3 (subsubsection)\}\par
Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim.\par
\par
\\paragraph\{Heading on level 4 (paragraph)\}\par
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. \par
\par
\par
\\section\{Lists\}\par
\\subsection\{Example for list (3*itemize)\}\par
\\begin\{itemize\}\par
\tab\\item First item in a list \par
\tab\tab\\begin\{itemize\}\par
\tab\tab\\item First item in a list \par
\tab\tab\tab\\begin\{itemize\}\par
\tab\tab\tab\\item First item in a list \par
\tab\tab\tab\\item Second item in a list \par
\tab\tab\tab\\end\{itemize\}\par
\tab\tab\\item Second item in a list \par
\tab\tab\\end\{itemize\}\par
\tab\\item Second item in a list \par
\\end\{itemize\}\par
\par
\\subsection\{Example for list (enumerate)\}\par
\\begin\{enumerate\}\par
\tab\\item First item in a list \par
\tab\\item Second item in a list \par
\tab\\item Third item in a list\par
\\end\{enumerate\}\par
%%% End document\par
\\end\{document\}\par
}
 